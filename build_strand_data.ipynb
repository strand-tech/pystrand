{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, date, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import sys\n",
    "from tiingo import TiingoClient\n",
    "\n",
    "config = {}\n",
    "config['session'] = True\n",
    "config['api_key'] = \"\"\n",
    "client = TiingoClient(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish dict for stock ticker to cik conversion\n",
    "ticker_cik = pd.read_csv(\"https://www.sec.gov/include/ticker.txt\", sep = \"\\t\", header = None)\n",
    "cik_dict = ticker_cik.set_index(0).to_dict()[1]\n",
    "\n",
    "# Function to establish the filing index pages\n",
    "def get_index_url(symbol, date):\n",
    "    # Get cik number for given symbol.\n",
    "    cik = cik_dict[symbol]\n",
    "    \n",
    "    # Initialize filing_date arbitrarily early date for use later.\n",
    "    filing_date = datetime.strptime('2000-01-01', '%Y-%m-%d').date()\n",
    "    \n",
    "    # Our goal is to find most recently filed corporate info - \n",
    "    # could be in a 10-Q or 10-K so we check both.\n",
    "    for filing in ['10-Q', '10-K']:\n",
    "        \n",
    "        # Find SEC filings page for current symbol, ticker, and date using wildcards.\n",
    "        base_url = \"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={}&type={}&dateb={}&owner=include&count=100\"\n",
    "        filing_page_resp = requests.get(base_url.format(cik, filing, date))\n",
    "        filing_page_str = filing_page_resp.text\n",
    "\n",
    "        # BS allows us to parse the HTML. Establish that HTML 'soup',\n",
    "        # find table of filings by class, and establish table rows by tag.\n",
    "        soup = BeautifulSoup(filing_page_str, 'html.parser')\n",
    "        table = soup.find('table', class_='tableFile2')\n",
    "        rows = table.find_all('tr')\n",
    "        \n",
    "        # Iterate over table rows, getting cells by tag. We break\n",
    "        # once we make sure that we find the right filing (and we\n",
    "        # check to ensure the right filing mostly because we could \n",
    "        # get mixed up by '10-K/A' amendments which appear in '10-K' \n",
    "        # search but aren't useful).\n",
    "        for row in rows:\n",
    "            cells = row.find_all('td')\n",
    "            # If this is True, we've reached table observations and\n",
    "            # found a relevant filing.\n",
    "            # We want to only use information we could have known\n",
    "            # PRIOR to the trading day, so I'll move past filings\n",
    "            # that were submitted on the same day as the input date.\n",
    "            if len(cells) > 3 and cells[0].text == filing and cells[3].text != date:\n",
    "                # We'll update link and filing_date IF date (cells[3]) is more\n",
    "                # recent than saved filing_date (to find most recent\n",
    "                # filing between 10-Q and 10-K pages).\n",
    "                check_date = datetime.strptime(cells[3].text, '%Y-%m-%d').date()\n",
    "                if check_date > filing_date:\n",
    "                    index_url = 'https://www.sec.gov' + cells[1].a['href']\n",
    "                    filing_date = check_date\n",
    "                    filing_type = filing\n",
    "                break\n",
    "                    \n",
    "    return(index_url, filing_date, filing_type)\n",
    "           \n",
    "\n",
    "# Function to establish relevant xbrl links.\n",
    "# All we need to do is pull index page for most recent company filing\n",
    "# and find the 'XBRL INSTANCE DOCUMENT'.\n",
    "def get_xbrl_url(index_url):    \n",
    "\n",
    "    # As in get_index_url() function, establish page text.\n",
    "    index_resp = requests.get(index_url)\n",
    "    index_str = index_resp.text\n",
    "    \n",
    "    # As in get_index_url() function, get BS 'soup', find our table\n",
    "    # of interest (in this case, doing so by table's summary), and\n",
    "    # establish rows.\n",
    "    soup = BeautifulSoup(index_str, \"html.parser\")\n",
    "    table = soup.find('table', summary='Data Files')\n",
    "    rows = table.find_all('tr')\n",
    "    \n",
    "    # Iterate over table rows looking for XBRL instance doc, saving link\n",
    "    # once we find it.\n",
    "    for row in rows:\n",
    "        cells = row.find_all('td')\n",
    "        if len(cells) > 3 and 'XBRL INSTANCE DOCUMENT' in cells[1].text:\n",
    "            xbrl_link = 'https://www.sec.gov' + cells[2].a['href']\n",
    "                \n",
    "    return(xbrl_link)\n",
    "\n",
    "\n",
    "# Get most recent values for our desired tags within the xbrl file.\n",
    "def get_tag_values(xbrl_link):\n",
    "\n",
    "    # As above, get XBRL text.\n",
    "    xbrl_resp = requests.get(xbrl_link)\n",
    "    xbrl_str = xbrl_resp.text\n",
    "    \n",
    "    # This time, our 'soup' is 'lxml' format instead of 'html'\n",
    "    soup = BeautifulSoup(xbrl_str, 'lxml')\n",
    "    \n",
    "    # At some stage, companies swapped from manual submission\n",
    "    # of their 'XBRL INSTANCE DOCUMENT' to iXBRL submission, resulting\n",
    "    # in 'EXTRACTED XBRL INSTANCE DOCUMENT's. The only relevant change \n",
    "    # this causes is that a tag's 'xbrli:context' became 'context' and\n",
    "    # the 'xbrli:enddate' became 'enddate'. We'll set these conditional\n",
    "    # on which version the file is using, as identified by whether we\n",
    "    # find an 'xbrli:context' tag in the file.\n",
    "    xbrli_string = ''\n",
    "    if soup.find('xbrli:context'):\n",
    "        xbrli_string = 'xbrli:'\n",
    "    \n",
    "    context_tag = xbrli_string + 'context'\n",
    "    enddate_tag = xbrli_string + 'enddate'\n",
    "    instant_tag = xbrli_string + 'instant'\n",
    "    \n",
    "    # Here, we'll pull the filing's reference period end\n",
    "    # (only for the sake of additional detail)\n",
    "    ref_period = soup.find('dei:documentfiscalperiodfocus')\n",
    "    date_id = ref_period.get('contextref')\n",
    "    period_context = soup.find(context_tag, {'id':date_id})\n",
    "    period_end = period_context.find(enddate_tag).text\n",
    "    \n",
    "    \n",
    "    # Now we get into pulling our equity values. Some key notes:\n",
    "    \n",
    "    # 1. We are only interested in these tags if they are true values, \n",
    "    # not 'dimensions' of those values. We check this by ensuring\n",
    "    # the tag's context does NOT contain an 'explicitmember' field.\n",
    "    \n",
    "    # 2. We only want to keep the most recent value (found in the tag's \n",
    "    # context), because filings can list values for past filings as well.\n",
    "    \n",
    "    # 3. We are interested in equity not including minority interest.\n",
    "    # 'stockholdersequity' =\n",
    "    # 'stockholdersequityincludingportionattributabletononcontrollinginterest'\n",
    "    # less 'minorityinterest'. If we are missing 'stockholdersequity', we can\n",
    "    # therefore calculate it with the other two figrues. In some cases, companies will\n",
    "    # list only 'stockholdersequityincludingportionattributabletononcontrollinginterest'\n",
    "    # without the other two figures. In these cases, it appears that the company does\n",
    "    # not have any 'minorityinterest' and we can use this value as our equity figure.\n",
    "    \n",
    "    # For more details, see:\n",
    "    # - https://xbrl.us/data-rule/dqc_0004pr/\n",
    "    # - https://xbrl.us/guidance/specific-non-controlling-interest-elements/\n",
    "    \n",
    "    # Here, we'll get the filing's most recent values for each tag and save to dict.\n",
    "    equity_dict = {}\n",
    "    for equity_tag in [\"us-gaap:stockholdersequity\", \"us-gaap:stockholdersequityincludingportionattributabletononcontrollinginterest\", \"us-gaap:minorityinterest\"]:\n",
    "        tag_dict = {\"date\": None,\n",
    "                   \"value\": None}\n",
    "        \n",
    "        # Find tags in document.\n",
    "        tag_list = soup.find_all(equity_tag)\n",
    "        \n",
    "        # Initialize for use in FOR loop.\n",
    "        equity_date = datetime.strptime('2000-01-01', '%Y-%m-%d').date()\n",
    "        \n",
    "        for t in tag_list:\n",
    "            # Get the tag's context for additional detail.\n",
    "            ref_id = t.get('contextref')\n",
    "            context = soup.find(context_tag, {'id':ref_id})\n",
    "            \n",
    "            # Pull the value's date reference from its context.\n",
    "            equity_date_obs = datetime.strptime(context.find(instant_tag).text, '%Y-%m-%d').date()\n",
    "            \n",
    "            # If the value is more recent than saved value AND NOT a 'dimension',\n",
    "            # we update the reference date and our saved value.\n",
    "            if equity_date_obs >= equity_date and not context.find('xbrldi:explicitmember'):\n",
    "                equity_date = equity_date_obs\n",
    "                tag_dict = {\"date\": equity_date,\n",
    "                           \"value\": int(float(t.text))}\n",
    "                \n",
    "        # Coming out of the above loop, we now have the 'value' of\n",
    "        # the most recent non-'dimension' tag and its 'date'. We save \n",
    "        # these, and the tag name for reference, to the equity_dict.\n",
    "        equity_dict.update({equity_tag: tag_dict})\n",
    "        \n",
    "    # To go alongside the logic in point (3.), we'll make sure to use the most recent\n",
    "    # equity figure if the figures' dates including and excluding minority interest differ.\n",
    "    # This did not occur in any of my trials but it seems like an edge case worth covering\n",
    "    # since I've found no explicit rules against it.\n",
    "    most_recent = True\n",
    "    if equity_dict[\"us-gaap:stockholdersequity\"]['value'] is not None and equity_dict[\"us-gaap:stockholdersequityincludingportionattributabletononcontrollinginterest\"]['value'] is not None:\n",
    "        if equity_dict[\"us-gaap:stockholdersequityincludingportionattributabletononcontrollinginterest\"]['date'] >= equity_dict[\"us-gaap:stockholdersequity\"]['date']:\n",
    "            most_recent = False\n",
    "    \n",
    "    # So, with our equity_dict, we can save a single equity value\n",
    "    # using the logic described in point (3.) above.\n",
    "    if equity_dict[\"us-gaap:stockholdersequity\"]['value'] is not None and most_recent:\n",
    "        equity = equity_dict[\"us-gaap:stockholdersequity\"]['value']\n",
    "        equity_date = equity_dict[\"us-gaap:stockholdersequity\"]['date']\n",
    "    else:\n",
    "        if equity_dict[\"us-gaap:minorityinterest\"]['value'] is not None:\n",
    "            equity = equity_dict[\"us-gaap:stockholdersequityincludingportionattributabletononcontrollinginterest\"]['value'] - equity_dict[\"us-gaap:minorityinterest\"]['value']\n",
    "        else:\n",
    "            equity = equity_dict[\"us-gaap:stockholdersequityincludingportionattributabletononcontrollinginterest\"]['value']\n",
    "        equity_date = equity_dict[\"us-gaap:stockholdersequityincludingportionattributabletononcontrollinginterest\"]['date']\n",
    "    \n",
    "    # Now we'll look to establish the shares outstanding figure.\n",
    "    # This is a bit simpler. According to the XBRL rules, each\n",
    "    # class of stock should have one and only one value in the \n",
    "    # document tagged 'entitycommonstocksharesoutstanding'. We \n",
    "    # can run through these tags and if we find a 'dimension'-less\n",
    "    # value we save and stop (this is the total of shares outstanding),\n",
    "    # and if we don't then we sum the 'dimension'-al values. We only\n",
    "    # do not blindly sum all tags to handle the case where a \n",
    "    # 'dimension'-less sum AND class-wise aspects are included \n",
    "    # (I don't think this should happen, but again I've found no\n",
    "    # explicit rule against it).\n",
    "    \n",
    "    # For additional detail, see:\n",
    "    # - https://www.sec.gov/structureddata/edgarvalidationerrors\n",
    "    \n",
    "    for shares_tag in [\"dei:entitycommonstocksharesoutstanding\"]:\n",
    "        # Establish tag list\n",
    "        tag_list = soup.find_all(shares_tag)\n",
    "        \n",
    "        shares = 0\n",
    "        \n",
    "        # Iterate over tag list\n",
    "        for t in tag_list:\n",
    "            ref_id = t.get('contextref')\n",
    "            context = soup.find(context_tag, {'id':ref_id})\n",
    "            \n",
    "            # If we find a 'dimension'-less value, this contains the total\n",
    "            # shares so we can save and break\n",
    "            if not context.find('xbrldi:explicitmember'):\n",
    "                shares = int(float(t.text))\n",
    "                shares_date = datetime.strptime(context.find(instant_tag).text, '%Y-%m-%d').date()\n",
    "                break\n",
    "                \n",
    "            # If we only find 'dimension'-al values, we'll sum them.\n",
    "            else:\n",
    "                shares += int(float(t.text))\n",
    "                shares_date = datetime.strptime(context.find(instant_tag).text, '%Y-%m-%d').date()\n",
    "        \n",
    "    # Of note - we save the date of the shares observation for\n",
    "    # later market cap calculations. And we save equity_date\n",
    "    # for completeness - Though this seems unnecessary because\n",
    "    # it appears to always be the same as period_end.\n",
    "    return(period_end, equity, equity_date, shares, shares_date)\n",
    "\n",
    "\n",
    "# Iterate over symbols and end dates to bring together\n",
    "# our sample_inputs (though we'll need to update with\n",
    "# prices to get market cap).\n",
    "def get_sample_inputs(symbols, dates):\n",
    "    \n",
    "    sample_inputs = pd.DataFrame()\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        for date in dates:\n",
    "            # Get index url for the filing we want w/ filing information\n",
    "            index_url, filing_date, filing_type = get_index_url(symbol, date)\n",
    "            # Get xbrl url from index page 'index_url'\n",
    "            xbrl_url = get_xbrl_url(index_url)\n",
    "            # Get tag values from the XBRL instance file 'xbrl_url'\n",
    "            period_end, equity, equity_date, shares, shares_date = get_tag_values(xbrl_url)\n",
    "            \n",
    "            # Get share price on most recent trading day prior to shares count observation.\n",
    "            # Using Tiingo API (requires session config as described at:\n",
    "            # https://tiingo-python.readthedocs.io/en/latest/readme.html#further-docs).\n",
    "            # Tiingo will throw an error if the date we try has no pricing data. So\n",
    "            # when the function fails we go back a day and try again.\n",
    "            shares_date_check = shares_date\n",
    "            while True:\n",
    "                try:\n",
    "                    share_price = client.get_dataframe(symbol, \n",
    "                                                       metric_name = 'close',\n",
    "                                                       startDate = shares_date_check,\n",
    "                                                       endDate = shares_date_check)[0]\n",
    "                    break\n",
    "                except:\n",
    "                    shares_date_check = shares_date_check - timedelta(days=1)\n",
    "                \n",
    "            \n",
    "            # Calculate market cap on the date of the shares observation. \n",
    "            market_cap = shares * share_price\n",
    "            \n",
    "            # Bring values together into dataframe rows and append to full frame.\n",
    "            row_values = [symbol, cik_dict[symbol], date, filing_date, period_end, filing_type, equity, equity_date, shares, shares_date, market_cap, index_url, xbrl_url, datetime.now()]\n",
    "            names = ['symbol', 'cik', 'end_date', 'filing_date', 'period_end', 'filing_type', 'total_equity', 'equity_date', 'shares_outstanding', 'shares_date', 'market_cap', 'index_url', 'xbrl_url', 'download_datetime']\n",
    "            row = pd.DataFrame([row_values], columns=names)\n",
    "            sample_inputs = sample_inputs.append(row)\n",
    "            \n",
    "    return(sample_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_pricing(symbols, date_range):\n",
    "    \n",
    "    # Establish dates for Tiingo use.\n",
    "    start = date_range[0]\n",
    "    end = date_range[1]\n",
    "    \n",
    "    # Establish empty input_prices for reference when building the\n",
    "    # input_prices frame.\n",
    "    sample_pricing = pd.DataFrame(columns = ['symbol', 'date'])\n",
    "    \n",
    "    # Using Tiingo, we can either get all metrics for one symbol at\n",
    "    # a time or get one metric for many symbols at once. It feels\n",
    "    # more sensible to loop through our 4 metrics of interest than\n",
    "    # to loop through each symbol and select out those metrics.\n",
    "    for metric in ['close', 'volume', 'divCash', 'splitFactor']:\n",
    "        # Get data by metric using Tiingo API (requires session\n",
    "        # config as described at:\n",
    "        # https://tiingo-python.readthedocs.io/en/latest/readme.html#further-docs)\n",
    "        wide_frame = client.get_dataframe(symbols,\n",
    "                                          metric_name = metric,\n",
    "                                          startDate = start,\n",
    "                                          endDate = end)\n",
    "\n",
    "        # Reset index for ease of use and rename (index is untitled\n",
    "        # if only one symbol is input). Then, melt the data to create\n",
    "        # observations by date and symbol. Finally, reformat dates.\n",
    "        wide_frame = wide_frame.reset_index().rename(columns = {'index':'date'})\n",
    "        metric_df = wide_frame.melt(id_vars = \"date\").rename(columns = {'variable':'symbol', 'value':metric})\n",
    "        metric_df['date'] = metric_df['date'].dt.date\n",
    "        \n",
    "        # Merge the metric frame to the full frame.\n",
    "        sample_pricing = sample_pricing.merge(metric_df, on = ['symbol', 'date'], how = 'outer')\n",
    "            \n",
    "    return sample_pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function to build the datasets.\n",
    "# `symbols` is a list of stock tickers.\n",
    "# `daterange` is a list (of 2) of a start date and end date for the daily data.\n",
    "# `end_dates` is a list of dates prior to which we'll scrape attribute data from XBRL.\n",
    "def build_data(symbols, date_range, end_dates):\n",
    "    sample_inputs = get_sample_inputs(symbols, end_dates)[['symbol', 'cik', 'total_equity', 'market_cap', 'end_date']]\n",
    "    sample_pricing = get_sample_pricing(symbols, date_range)\n",
    "    \n",
    "    return sample_inputs, sample_pricing\n",
    "    \n",
    "# sample_inputs, sample_pricing = build_data(['aapl', 'msft'], ['2020-06-01', '2020-06-30'], ['2020-06-01'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
